{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzmoxerWJmnwXJJKFT/vhA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlosks/carlosks/blob/main/Dividir_Arquivo_Desafio_Limitado_20M_cada.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Conexão com o Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0s6EQsWAbRNC",
        "outputId": "b3428248-a5c4-47c7-f177-475dc9f4a001"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "import json\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# Função auxiliar para salvar parte do DataFrame no formato JSON\n",
        "\n",
        "def save_json_chunk(df_chunk, part_num, base_path):\n",
        "\n",
        "    file_name = f'{base_path}/TRN_Dataset_Part_{part_num}.json'\n",
        "\n",
        "    df_chunk.to_json(file_name, orient='records', lines=True)\n",
        "\n",
        "    return file_name\n",
        "\n",
        "\n",
        "\n",
        "# Função para dividir e salvar o DataFrame com limite de 20MB por arquivo\n",
        "\n",
        "def split_dataframe_to_json(df_filtered, base_path, max_file_size_mb=20):\n",
        "\n",
        "    max_file_size_bytes = max_file_size_mb * 1024 * 1024  # Converter MB para bytes\n",
        "\n",
        "    part_num = 1\n",
        "\n",
        "    chunk_size = 1000  # Número de linhas processadas de cada vez\n",
        "\n",
        "    start_idx = 0\n",
        "\n",
        "\n",
        "\n",
        "    # Inicializa um DataFrame vazio para armazenar os dados temporariamente\n",
        "\n",
        "    df_chunk = pd.DataFrame()\n",
        "\n",
        "\n",
        "\n",
        "    while start_idx < len(df_filtered):\n",
        "\n",
        "        # Adiciona um pedaço do DataFrame ao chunk\n",
        "\n",
        "        df_chunk = pd.concat([df_chunk, df_filtered.iloc[start_idx:start_idx + chunk_size]])\n",
        "\n",
        "        # Salva o chunk temporariamente em um arquivo para medir o tamanho\n",
        "\n",
        "        file_name = save_json_chunk(df_chunk, part_num, base_path)\n",
        "\n",
        "\n",
        "\n",
        "        # Verifica o tamanho do arquivo\n",
        "\n",
        "        file_size = os.path.getsize(file_name)\n",
        "\n",
        "\n",
        "\n",
        "        if file_size >= max_file_size_bytes:\n",
        "\n",
        "            # Se o arquivo exceder o tamanho, começa um novo arquivo e aumenta o número da parte\n",
        "\n",
        "            print(f\"Arquivo {file_name} salvo com {file_size / (1024 * 1024):.2f} MB.\")\n",
        "\n",
        "            part_num += 1\n",
        "\n",
        "            df_chunk = pd.DataFrame()  # Limpa o DataFrame temporário para iniciar novo arquivo\n",
        "\n",
        "\n",
        "\n",
        "        start_idx += chunk_size\n",
        "\n",
        "\n",
        "\n",
        "    # Salva qualquer dado restante que não tenha sido salvo\n",
        "\n",
        "    if not df_chunk.empty:\n",
        "\n",
        "        file_name = save_json_chunk(df_chunk, part_num, base_path)\n",
        "\n",
        "        print(f\"Arquivo {file_name} salvo com {os.path.getsize(file_name) / (1024 * 1024):.2f} MB.\")\n",
        "\n",
        "\n",
        "\n",
        "# Carregar o JSON de um arquivo ou string\n",
        "\n",
        "with open('/content/drive/MyDrive/Desafio_Fase_3/trn.json', 'r') as f:\n",
        "\n",
        "    file_content = f.read()\n",
        "\n",
        "    try:\n",
        "\n",
        "        dados_json = json.loads(file_content)\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "\n",
        "        dados_json = [json.loads(line) for line in file_content.splitlines() if line.strip()]\n",
        "\n",
        "\n",
        "\n",
        "# Converter o JSON em um DataFrame normalizado\n",
        "\n",
        "df = pd.json_normalize(dados_json)\n",
        "\n",
        "\n",
        "\n",
        "# Filtrar registros cujo uid seja numérico\n",
        "\n",
        "try:\n",
        "\n",
        "    df_filtered = df[df['uid'].apply(lambda x: str(x).isdigit())]\n",
        "\n",
        "except KeyError:\n",
        "\n",
        "    df_filtered = df.drop(columns=['uid'], errors='ignore')\n",
        "\n",
        "\n",
        "\n",
        "# Filtrar registros onde 'content' não está vazio\n",
        "\n",
        "try:\n",
        "\n",
        "    df_filtered = df_filtered[df_filtered['content'].notna() & (df_filtered['content'].str.strip() != '')]\n",
        "\n",
        "except KeyError:\n",
        "\n",
        "    df_filtered = df_filtered.drop(columns=['content'], errors='ignore')\n",
        "\n",
        "\n",
        "\n",
        "# Manter apenas as colunas 'title' e 'content'\n",
        "\n",
        "df_filtered = df_filtered[['title', 'content']]\n",
        "\n",
        "\n",
        "\n",
        "# Dividir e salvar o DataFrame em arquivos de no máximo 20 MB\n",
        "\n",
        "split_dataframe_to_json(df_filtered, base_path='/content/drive/MyDrive/Desafio_Fase_3', max_file_size_mb=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQREyw7-bYQ7",
        "outputId": "0c02ab02-20f6-4baf-9b66-039c3e71d517"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo /content/drive/MyDrive/Desafio_Fase_3/TRN_Dataset_Part_1.json salvo com 20.26 MB.\n",
            "Arquivo /content/drive/MyDrive/Desafio_Fase_3/TRN_Dataset_Part_2.json salvo com 20.56 MB.\n",
            "Arquivo /content/drive/MyDrive/Desafio_Fase_3/TRN_Dataset_Part_3.json salvo com 20.63 MB.\n",
            "Arquivo /content/drive/MyDrive/Desafio_Fase_3/TRN_Dataset_Part_4.json salvo com 20.60 MB.\n",
            "Arquivo /content/drive/MyDrive/Desafio_Fase_3/TRN_Dataset_Part_5.json salvo com 20.06 MB.\n",
            "Arquivo /content/drive/MyDrive/Desafio_Fase_3/TRN_Dataset_Part_6.json salvo com 21.10 MB.\n",
            "Arquivo /content/drive/MyDrive/Desafio_Fase_3/TRN_Dataset_Part_7.json salvo com 20.01 MB.\n",
            "Arquivo /content/drive/MyDrive/Desafio_Fase_3/TRN_Dataset_Part_8.json salvo com 20.64 MB.\n",
            "Arquivo /content/drive/MyDrive/Desafio_Fase_3/TRN_Dataset_Part_9.json salvo com 20.24 MB.\n",
            "Arquivo /content/drive/MyDrive/Desafio_Fase_3/TRN_Dataset_Part_10.json salvo com 20.37 MB.\n",
            "Arquivo /content/drive/MyDrive/Desafio_Fase_3/TRN_Dataset_Part_11.json salvo com 21.01 MB.\n",
            "Arquivo /content/drive/MyDrive/Desafio_Fase_3/TRN_Dataset_Part_12.json salvo com 20.28 MB.\n",
            "Arquivo /content/drive/MyDrive/Desafio_Fase_3/TRN_Dataset_Part_13.json salvo com 20.48 MB.\n",
            "Arquivo /content/drive/MyDrive/Desafio_Fase_3/TRN_Dataset_Part_14.json salvo com 20.55 MB.\n",
            "Arquivo /content/drive/MyDrive/Desafio_Fase_3/TRN_Dataset_Part_15.json salvo com 20.76 MB.\n",
            "Arquivo /content/drive/MyDrive/Desafio_Fase_3/TRN_Dataset_Part_16.json salvo com 20.39 MB.\n",
            "Arquivo /content/drive/MyDrive/Desafio_Fase_3/TRN_Dataset_Part_17.json salvo com 20.36 MB.\n",
            "Arquivo /content/drive/MyDrive/Desafio_Fase_3/TRN_Dataset_Part_18.json salvo com 4.37 MB.\n"
          ]
        }
      ]
    }
  ]
}